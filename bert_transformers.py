# -*- coding: utf-8 -*-
"""Bert Transformers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u10S3YYIKtSKTlQjp8-lz1KfUOiTKW6p
"""

#!pip install transformers

import numpy as np
import pandas as pd
pd.set_option('max_colwidth',None)
import seaborn as sns
import matplotlib.pyplot as plt
import re
import string
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import SpatialDropout1D, Dense, Embedding, SimpleRNN, LSTM, Bidirectional, Dropout
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from sklearn.model_selection import train_test_split
from nltk import word_tokenize
# Importing the Random Forest model
from sklearn.ensemble import RandomForestClassifier

# Metrics to evaluate the model
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Function to print the classification report and get confusion matrix in a proper format

def metrics_score(actual, predicted):

    print(classification_report(actual, predicted))

    cm = confusion_matrix(actual, predicted)

    plt.figure(figsize = (8, 5))

    sns.heatmap(cm, annot = True,  fmt = '.2f', xticklabels = ['negative', 'positive'], yticklabels = ['negative', 'positive'])

    plt.ylabel('Actual')

    plt.xlabel('Predicted')

    plt.show()

df=pd.read_csv('/content/Movieupdate.csv')

df.head()

df.shape

"""## Implementing Bert"""

X= df['text']
y=df['label']

X_train, X_test, y_train, y_test= train_test_split(X,y, stratify=y, test_size=0.2, random_state=0)

from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler

#load the DistilBert tokenizer and model
tokenizer=DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
bert_model=DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)

#testing
sample_tweet=X_train[0]
inputs=tokenizer(sample_tweet, padding=True, truncation=True, return_tensors='pt')

inputs

def preprocess_data(tweet, label, tokenizer):
  inputs=tokenizer(tweet.tolist(), padding=True, truncation=True, return_tensors='pt')
  labels=torch.tensor(label.tolist())
  dataset= TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)
  return dataset

train_dataset =preprocess_data(X_train, y_train, tokenizer)
test_dataset =preprocess_data(X_test, y_test, tokenizer)

batch_size=32

#sample the datset randomly
train_sampler=RandomSampler(train_dataset)
train_dataloader =DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)

#Set up the optimizer
optimizer=torch.optim.AdamW(bert_model.parameters(), lr=1e-5)

#Set gradient accumulation
gradient_accumulation =4

#Set the training loop
bert_model.train()

for epoch in range (2):
  print(f'===========Epoch: {epoch+1}==============')
  total_loss =0.0
  for step, batch in enumerate(train_dataloader):
    #CLEAR ANY GRADIENT ALREADY ACCUMULATED
    optimizer.zero_grad()
    outputs=bert_model(batch[0], attention_mask=batch[1], labels=batch[2])
    #compute loss
    loss=output.loss
    loss=loss/gradient-gradient_accumulation
    loss.backward()
    if (step+1) % gradient_accumulation == 0:
      optimizer.step()
      total_loss +=loss.item()
      print(f'---------Adjusted weights after {step + 1} steps ----------')
  print(f'Epoch: {epoch + 1} - Average Loss: {total_loss/len(train_dataloader)}')